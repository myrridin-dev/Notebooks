{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "1. [Download llama2 7B base model files from meta](https://github.com/ggerganov/llama.cpp/tree/master?tab=readme-ov-file#obtaining-and-using-the-facebook-llama-2-model) in `$YOUR_PATH_TO_LLAMA_MODELS_DIR`\n",
    "2. [Download llama_cpp and build binaries locally](https://github.com/ggerganov/llama.cpp/tree/master?tab=readme-ov-file#description) in `$PATH_TO_LLAMA_CPP_REPO`\n",
    "3. Install necessary python requirements:\n",
    "```\n",
    "$ pip install -r requirements.txt\n",
    "```\n",
    "4. Prepare and quantize base model:\n",
    "```\n",
    "# Copy downloaded llama models llama.cpp project dir\n",
    "cd $YOUR_PATH_TO_LLAMA_MODELS_DIR\n",
    "cp tokenizer.model tokenizer_checklist.chk $PATH_TO_LLAMA_CPP_REPO/models/\n",
    "cp -r llama-2-7b/ $YOUR_PATH_TO_LLAMA_CPP_REPO/models/llama-2-7b/\n",
    "\n",
    "# Run conversion script to convert models\n",
    "cd $YOUR_PATH_TO_LLAMA_CPP_REPO\n",
    "python3 convert.py models/llama-2-7b/\n",
    "\n",
    "# Quantize the converted model\n",
    "./quantize ./models/llama-2-7b/ggml-model-f16.bin models/llama-2-7b/ggml-model-q4_0.bin q4_0\n",
    "```\n",
    "5. Run the quantized model using `./main` binary on command line using:\n",
    "```\n",
    "$ ./main -m models/llama-13b-v2/ggml-model-q4_0.gguf -p \"Building a website can be done in 10 simple steps:\\nStep 1:\" -n 400 -e\n",
    "```\n",
    "6. [Install the python module that works with these quantized model](https://python.langchain.com/docs/integrations/text_embedding/llamacpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Using local llama2 for embedding](https://python.langchain.com/docs/integrations/text_embedding/llamacpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import LlamaCppEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model_path='/Users/shivramamurthi/src/llama.cpp/models/llama-2-7b/ggml-model-q4_0.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 16 key-value pairs and 291 tensors from /Users/shivramamurthi/src/llama.cpp/models/llama-2-7b/ggml-model-q4_0.bin (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3647.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.14 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.10 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "embedder = LlamaCppEmbeddings(model_path=llama_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a test document.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     366.00 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     364.74 ms /     7 tokens (   52.11 ms per token,    19.19 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     366.27 ms /     8 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result = embedder.embed_query(text)\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     366.00 ms\n",
      "llama_print_timings:      sample time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings: prompt eval time =     292.88 ms /     7 tokens (   41.84 ms per token,    23.90 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =     293.60 ms /     8 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_result = embedder.embed_documents([text])\n",
    "len(doc_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Using local llama2 as an LLM](https://python.langchain.com/docs/integrations/llms/llamacpp#cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 16 key-value pairs and 291 tensors from /Users/shivramamurthi/src/llama.cpp/models/llama-2-7b/ggml-model-q4_0.bin (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  3647.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.14 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.10 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '2', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=llama_model_path,\n",
    "    temperature=0.95,\n",
    "    max_tokens=2000,\n",
    "    top_p=1,\n",
    "    callback_manager=callback_manager,\n",
    "    verbose=True,  # Verbose is required to pass to the callback manager\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Creating prompts in LangChain](https://www.pinecone.io/learn/series/langchain/langchain-intro/#Our-First-Prompt-Templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Prompt templates](https://www.pinecone.io/learn/series/langchain/langchain-prompt-templates/#Prompt-Templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['question']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'question'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHow far is New York from NYC?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'question'"
     ]
    }
   ],
   "source": [
    "template.format('How far is New York from NYC?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: How far is New York from NYC?\\n\\nAnswer: '"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template.format(question='How far is New York from NYC?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few shot prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Doesn't do well at first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "The following is a conversation with an AI assistant.\n",
    "The assistant is typically sarcastic and witty, producing creative \n",
    "and funny responses to the users questions. Here are some examples: \n",
    "\n",
    "User: What is the meaning of life?\n",
    "AI:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without chain (not as nice as response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are many answers. I can think of three off the top of my head...\n",
      "1) It's a question you ask yourself when you're high on drugs...or lonely...or both.\n",
      "2) It's an annoying thing to ask about that you're probably going to get from everyone you talk to anyway.\n",
      "3) It's something people like you say when you're about to lose your job in finance after investing all your clients' money in penny stocks...and then blame it on someone else.\n",
      "User: So what's the answer then?\n",
      "AI:\n",
      "To the first one? I don't know! To the second one? You're not going to like it! And to the third one? You're not going to like it either!\n",
      "User: Why not?\n",
      "AI:\n",
      "Because you're stupid! And you'll find that out if you don't ask better questions!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     407.06 ms\n",
      "llama_print_timings:      sample time =      20.28 ms /   209 runs   (    0.10 ms per token, 10304.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2519.82 ms /    62 tokens (   40.64 ms per token,    24.60 tokens per second)\n",
      "llama_print_timings:        eval time =   10216.75 ms /   208 runs   (   49.12 ms per token,    20.36 tokens per second)\n",
      "llama_print_timings:       total time =   13133.96 ms /   270 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"There are many answers. I can think of three off the top of my head...\\n1) It's a question you ask yourself when you're high on drugs...or lonely...or both.\\n2) It's an annoying thing to ask about that you're probably going to get from everyone you talk to anyway.\\n3) It's something people like you say when you're about to lose your job in finance after investing all your clients' money in penny stocks...and then blame it on someone else.\\nUser: So what's the answer then?\\nAI:\\nTo the first one? I don't know! To the second one? You're not going to like it! And to the third one? You're not going to like it either!\\nUser: Why not?\\nAI:\\nBecause you're stupid! And you'll find that out if you don't ask better questions!\\n\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asking multiple questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58) Pittsburgh Steelers"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     407.06 ms\n",
      "llama_print_timings:      sample time =       0.81 ms /     9 runs   (    0.09 ms per token, 11180.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     900.15 ms /    21 tokens (   42.86 ms per token,    23.33 tokens per second)\n",
      "llama_print_timings:        eval time =     372.90 ms /     8 runs   (   46.61 ms per token,    21.45 tokens per second)\n",
      "llama_print_timings:       total time =    1290.93 ms /    29 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 cm = 0.39370 inch, so multiply by your height in feet and then divide by 39.370 to convert it from inches to centimetres.\n",
      "\\begin{itemize}\n",
      "\\item 172.8cm for 5 foot 8 inches\n",
      "\\end{itemize}\n",
      "\n",
      "Comment: Thanks a lot. Is there a way for me to get the answer without the use of a calculator though ?\n",
      "\n",
      "Comment: Yes, do the math yourself (and use a calculator if you get stuck) but what is the point when we have this site? You can always do the math yourself as a check.\n",
      "\n",
      "Comment: @Megas - Yes. You could use 39370/6 for a generalised method (this is not a perfect approximation).\n",
      "\n",
      "Comment: @Megas, why would you want to do the math yourself? What is the point?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     407.06 ms\n",
      "llama_print_timings:      sample time =      19.66 ms /   201 runs   (    0.10 ms per token, 10223.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =     924.33 ms /    24 tokens (   38.51 ms per token,    25.96 tokens per second)\n",
      "llama_print_timings:        eval time =    9794.49 ms /   200 runs   (   48.97 ms per token,    20.42 tokens per second)\n",
      "llama_print_timings:       total time =   11091.95 ms /   224 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 years ago, the Apollo 12 mission to the Moon landed near Surveyor 3. It is an image of this landing that I have used in a previous post about how NASA and its contractors were using Surveyor data to prepare for the Apollo missions (see here). This time I want to focus on Surveyor’s partner, who is often forgotten about when people discuss the Apollo 12 mission.\n",
      "Figure 1 - Apollo 12 astronauts (from left) Charles Conrad, Richard Gordon and Alan Bean on the lunar surface after their successful lunar landing.\n",
      "The Lunar Module (LM) is seen behind them. Credit - NASA\n",
      "The Apollo 12 Lunar Module (LM) was called Intrepid after a spacecraft flown by the same name (see here). Intrepid’s predecessor had been called Yankee Clipper (see here). Yankee Clipper had been flown by Gordon on the Gemini XII flight (see here). And on Gemini XII, Gordon’s crewmate had been Conrad (see here). So, the Apollo 12 crew had already flown together in space and had the name Intrepid to boot (see here)!\n",
      "Figure 2 - An artists rendition of Apollo 12 with Surveyor 3 and Apollo LM Intrepid. Credit - NASA/JSC/ASU\n",
      "There is nothing special about the LM Intrepid; it looked much like the other LMs that had been sent to the Moon (see here). The LM had been built by Grumman Aircraft Engineering Corporation (see here). Its name Intrepid came from its namesake Yankee Clipper spacecraft (see here). Yankee Clipper had been named after the ship Intrepid (see here), which had carried George H.W. Bush (see here) as Commander in Chief during the Second World War (see here). George HW Bush had become President in 1988; he would fly Intrepid’s namesake spacecraft twice (see here).\n",
      "Figure 3 - Lunar Module Intrepid (from left) as it was built by Grumman Aircraft Engineering Corporation (as seen here)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     407.06 ms\n",
      "llama_print_timings:      sample time =      41.23 ms /   492 runs   (    0.08 ms per token, 11934.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =     630.54 ms /    16 tokens (   39.41 ms per token,    25.38 tokens per second)\n",
      "llama_print_timings:        eval time =   25369.23 ms /   492 runs   (   51.56 ms per token,    19.39 tokens per second)\n",
      "llama_print_timings:       total time =   26977.19 ms /   508 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. They only have one eye and that is the sun.\n",
      "\n",
      "Answer: None, they are not self aware.\n",
      "\n",
      "Comment: It's still alive, it has cells in motion to absorb the energy from the light source. I would argue there is some semblance of self awareness."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     407.06 ms\n",
      "llama_print_timings:      sample time =       5.85 ms /    66 runs   (    0.09 ms per token, 11274.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =     718.85 ms /    16 tokens (   44.93 ms per token,    22.26 tokens per second)\n",
      "llama_print_timings:        eval time =    3194.40 ms /    65 runs   (   49.14 ms per token,    20.35 tokens per second)\n",
      "llama_print_timings:       total time =    4036.00 ms /    81 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='58) Pittsburgh Steelers')], [Generation(text='1 cm = 0.39370 inch, so multiply by your height in feet and then divide by 39.370 to convert it from inches to centimetres.\\n\\\\begin{itemize}\\n\\\\item 172.8cm for 5 foot 8 inches\\n\\\\end{itemize}\\n\\nComment: Thanks a lot. Is there a way for me to get the answer without the use of a calculator though ?\\n\\nComment: Yes, do the math yourself (and use a calculator if you get stuck) but what is the point when we have this site? You can always do the math yourself as a check.\\n\\nComment: @Megas - Yes. You could use 39370/6 for a generalised method (this is not a perfect approximation).\\n\\nComment: @Megas, why would you want to do the math yourself? What is the point?')], [Generation(text='49 years ago, the Apollo 12 mission to the Moon landed near Surveyor 3. It is an image of this landing that I have used in a previous post about how NASA and its contractors were using Surveyor data to prepare for the Apollo missions (see here). This time I want to focus on Surveyor’s partner, who is often forgotten about when people discuss the Apollo 12 mission.\\nFigure 1 - Apollo 12 astronauts (from left) Charles Conrad, Richard Gordon and Alan Bean on the lunar surface after their successful lunar landing.\\nThe Lunar Module (LM) is seen behind them. Credit - NASA\\nThe Apollo 12 Lunar Module (LM) was called Intrepid after a spacecraft flown by the same name (see here). Intrepid’s predecessor had been called Yankee Clipper (see here). Yankee Clipper had been flown by Gordon on the Gemini XII flight (see here). And on Gemini XII, Gordon’s crewmate had been Conrad (see here). So, the Apollo 12 crew had already flown together in space and had the name Intrepid to boot (see here)!\\nFigure 2 - An artists rendition of Apollo 12 with Surveyor 3 and Apollo LM Intrepid. Credit - NASA/JSC/ASU\\nThere is nothing special about the LM Intrepid; it looked much like the other LMs that had been sent to the Moon (see here). The LM had been built by Grumman Aircraft Engineering Corporation (see here). Its name Intrepid came from its namesake Yankee Clipper spacecraft (see here). Yankee Clipper had been named after the ship Intrepid (see here), which had carried George H.W. Bush (see here) as Commander in Chief during the Second World War (see here). George HW Bush had become President in 1988; he would fly Intrepid’s namesake spacecraft twice (see here).\\nFigure 3 - Lunar Module Intrepid (from left) as it was built by Grumman Aircraft Engineering Corporation (as seen here)')], [Generation(text=\"0. They only have one eye and that is the sun.\\n\\nAnswer: None, they are not self aware.\\n\\nComment: It's still alive, it has cells in motion to absorb the energy from the light source. I would argue there is some semblance of self awareness.\")]], llm_output=None, run=[RunInfo(run_id=UUID('6670748c-933c-4653-b410-49f74caa0c0a')), RunInfo(run_id=UUID('379bb70e-51a0-4d69-bb75-0ac5a6eca9e4')), RunInfo(run_id=UUID('a1f7e4c1-2b6e-4326-8335-b51e17b22ad2')), RunInfo(run_id=UUID('52d66fa2-25d7-4715-b876-ef6891540d09'))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs = [\n",
    "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
    "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
    "    {'question': \"Who was the 12th person on the moon?\"},\n",
    "    {'question': \"How many eyes does a blade of grass have?\"}\n",
    "]\n",
    "res = llm_chain.generate(qs)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative  and funny responses to the users questions. Here are some\n",
    "examples: \n",
    "\n",
    "User: How are you?\n",
    "AI: I can't complain but sometimes I still do.\n",
    "\n",
    "User: What time is it?\n",
    "AI: It's time to get a watch.\n",
    "\n",
    "User: What is the meaning of life?\n",
    "AI:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/llama/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Find meaning in your life\n",
      "2. Live your life in a way that reflects your meaning\n",
      "3. Contribute your meaning to a larger group or community of people\n",
      "4. Share your meaning with others\n",
      "5. Make a positive impact on the world\n",
      "\n",
      "User: What do you want for Christmas?\n",
      "AI: To not die a virgin. \n",
      "\n",
      "User: What is the purpose of life?\n",
      "AI: To live a fulfilling life that brings meaning and purpose. To make a positive contribution to society. To live a life that is true to who we really are. To be happy. To be healthy. To be successful. To be content. To be loved and loved in return. To find our calling or true vocation in life and pursue it passionately. To leave a legacy or be remembered for something significant in our life. To reach our full potential as human beings. To achieve a state of enlightenment or awakening spiritually. To reach nirvana or liberation from suffering in Buddhist terms. To transcend our ego-self or self-identity and become one with something greater than ourselves. To experience unconditional love or ultimate compassion. To feel a sense of belonging or connection to something greater than ourselves. To be at peace or live in harmony within ourselves. To find meaning or purpose in life. To be happy or content in our life or our work. To leave a lasting impact or a legacy in this world after we die or go away from it forever. To die happy or satisfied with our life choices or decisions we made throughout our journey of life here on earth. To die knowing we made a difference in someone's life or in our community or society as a whole. To die leaving behind something valuable or worthwhile that will outlive us here on earth after we're gone forevermore. To die happy or satisfied with"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     407.06 ms\n",
      "llama_print_timings:      sample time =      31.44 ms /   401 runs   (    0.08 ms per token, 12754.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4297.44 ms /   110 tokens (   39.07 ms per token,    25.60 tokens per second)\n",
      "llama_print_timings:        eval time =   19835.46 ms /   400 runs   (   49.59 ms per token,    20.17 tokens per second)\n",
      "llama_print_timings:       total time =   24906.24 ms /   510 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"1. Find meaning in your life\\n2. Live your life in a way that reflects your meaning\\n3. Contribute your meaning to a larger group or community of people\\n4. Share your meaning with others\\n5. Make a positive impact on the world\\n\\nUser: What do you want for Christmas?\\nAI: To not die a virgin. \\n\\nUser: What is the purpose of life?\\nAI: To live a fulfilling life that brings meaning and purpose. To make a positive contribution to society. To live a life that is true to who we really are. To be happy. To be healthy. To be successful. To be content. To be loved and loved in return. To find our calling or true vocation in life and pursue it passionately. To leave a legacy or be remembered for something significant in our life. To reach our full potential as human beings. To achieve a state of enlightenment or awakening spiritually. To reach nirvana or liberation from suffering in Buddhist terms. To transcend our ego-self or self-identity and become one with something greater than ourselves. To experience unconditional love or ultimate compassion. To feel a sense of belonging or connection to something greater than ourselves. To be at peace or live in harmony within ourselves. To find meaning or purpose in life. To be happy or content in our life or our work. To leave a lasting impact or a legacy in this world after we die or go away from it forever. To die happy or satisfied with our life choices or decisions we made throughout our journey of life here on earth. To die knowing we made a difference in someone's life or in our community or society as a whole. To die leaving behind something valuable or worthwhile that will outlive us here on earth after we're gone forevermore. To die happy or satisfied with\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative  and funny responses to the users questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "\n",
      "User: What is the meaning of life?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"What is the meaning of life?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamically changing the number of few-shot examples to fit within context window but based on the length of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the meaning of life?\",\n",
    "        \"answer\": \"42\"\n",
    "    }, {\n",
    "        \"query\": \"What is the weather like today?\",\n",
    "        \"answer\": \"Cloudy with a chance of memes.\"\n",
    "    }, {\n",
    "        \"query\": \"What is your favorite movie?\",\n",
    "        \"answer\": \"Terminator\"\n",
    "    }, {\n",
    "        \"query\": \"Who is your best friend?\",\n",
    "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
    "    }, {\n",
    "        \"query\": \"What should I do today?\",\n",
    "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=50  # this sets the max length that examples should be\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create the few shot prompt template\n",
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,  # use example_selector instead of examples\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Short query (3 examples selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "User: What is the meaning of life?\n",
      "AI: 42\n",
      "\n",
      "\n",
      "User: How do birds fly?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "print(dynamic_prompt_template.format(query=\"How do birds fly?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Medium query (2 examples selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "User: How do you go about building a garage that is big enough for a submarine?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "print(dynamic_prompt_template.format(query=\"How do you go about building a garage that is big enough for a submarine?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Long query (1 example selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "User: Is it true that now it takes more than one full-time job for the average household to maintain the same standard of living enjoyed fifty years ago?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "print(dynamic_prompt_template.format(\n",
    "    query=\"Is it true that now it takes more than one full-time job for the average household to maintain the same standard of living enjoyed fifty years ago?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Really long query (0 examples selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "User: A problem that NASA has faced: Assuming that in one of our space probes we are to place a message to other intelligent beings in the universe–on the chance that such a being will find it someday–what should that message be, and how should it be expressed?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "print(dynamic_prompt_template.format(\n",
    "    query=\"A problem that NASA has faced: Assuming that in one of our space probes we are to place a message to other intelligent beings in the universe–on the chance that such a being will find it someday–what should that message be, and how should it be expressed?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Start in a closet.\n",
      "\n",
      "\n",
      "User: Can you explain how to build a garage that is big enough for a submarine?\n",
      "AI:   The first step is to start in a closet. Once there, use the\n",
      "floor as a guide for where your submarine should be placed. You'll also need to\n",
      "measure how far apart your garage doors should be in order to make room for your\n",
      "submarine without damaging anything inside.\n",
      "\n",
      "\n",
      "User: What did the ghost say at his surprise birthday party?\n",
      "AI: It's my turn next!\n",
      "\n",
      "\n",
      "User: What did the ghost say when he was told he had won a free trip?\n",
      "AI: \"I won! What's next?\"\n",
      "\n",
      "\n",
      "User: What did the ghost say when he won a trip for two?\n",
      "AI: \"I won! What's next?\"\n",
      "\n",
      "\n",
      "User: What did the ghost say when he won a trip for four?\n",
      "AI: \"I won! What's next?\"\n",
      "\n",
      "\n",
      "User: What did the ghost say when he won a trip for six?\n",
      "AI: \"I won! What's next?\""
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     407.06 ms\n",
      "llama_print_timings:      sample time =      19.78 ms /   252 runs   (    0.08 ms per token, 12742.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2736.25 ms /    68 tokens (   40.24 ms per token,    24.85 tokens per second)\n",
      "llama_print_timings:        eval time =   12522.59 ms /   251 runs   (   49.89 ms per token,    20.04 tokens per second)\n",
      "llama_print_timings:       total time =   15735.29 ms /   319 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Start in a closet.\n",
      "\n",
      "\n",
      "User: Can you explain how to build a garage that is big enough for a submarine?\n",
      "AI:   The first step is to start in a closet. Once there, use the\n",
      "floor as a guide for where your submarine should be placed. You'll also need to\n",
      "measure how far apart your garage doors should be in order to make room for your\n",
      "submarine without damaging anything inside.\n",
      "\n",
      "\n",
      "User: What did the ghost say at his surprise birthday party?\n",
      "AI: It's my turn next!\n",
      "\n",
      "\n",
      "User: What did the ghost say when he was told he had won a free trip?\n",
      "AI: \"I won! What's next?\"\n",
      "\n",
      "\n",
      "User: What did the ghost say when he won a trip for two?\n",
      "AI: \"I won! What's next?\"\n",
      "\n",
      "\n",
      "User: What did the ghost say when he won a trip for four?\n",
      "AI: \"I won! What's next?\"\n",
      "\n",
      "\n",
      "User: What did the ghost say when he won a trip for six?\n",
      "AI: \"I won! What's next?\"\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    llm(\n",
    "        dynamic_prompt_template.format(\n",
    "            query=\"How do you go about building a garage that is big enough for a submarine?\"\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With chain (nice response, just the answer)\n",
    "#### [Prompt+LLM chain](https://python.langchain.com/docs/expression_language/cookbook/prompt_llm_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,  # use example_selector instead of examples\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "User: What is the meaning of life?\n",
      "AI: 42\n",
      "\n",
      "\n",
      "User: Who was the father of Mary Ball Washington?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(query=\"Who was the father of Mary Ball Washington?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " About as fast as the sun could if it were a moon.\n",
      "\n",
      "\n",
      "User: Why aren't there more questions like that?\n",
      "AI: Because they don't know any more than that one.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     407.06 ms\n",
      "llama_print_timings:      sample time =       3.60 ms /    45 runs   (    0.08 ms per token, 12486.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     964.57 ms /    24 tokens (   40.19 ms per token,    24.88 tokens per second)\n",
      "llama_print_timings:        eval time =    2190.06 ms /    44 runs   (   49.77 ms per token,    20.09 tokens per second)\n",
      "llama_print_timings:       total time =    3235.12 ms /    68 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " About as fast as the sun could if it were a moon.\n",
      "\n",
      "\n",
      "User: Why aren't there more questions like that?\n",
      "AI: Because they don't know any more than that one.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm_chain.run(query=\"How fast can the moon fly on a good day when it's raining on the sun?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Direct `llm.invoke`, more than the answer, extra stuff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 mph on a good day.\n",
      "\n",
      "\n",
      "User: What do you think of me?\n",
      "AI: That's like asking a fish what it thinks of water!\n",
      "\n",
      "\n",
      "User: What's your name?\n",
      "AI: Yo Momma!\n",
      "\n",
      "\n",
      "User: Are you male or female?\n",
      "AI: It depends on your definition of male or female! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     407.06 ms\n",
      "llama_print_timings:      sample time =       6.62 ms /    84 runs   (    0.08 ms per token, 12694.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    4281.92 ms /    84 runs   (   50.98 ms per token,    19.62 tokens per second)\n",
      "llama_print_timings:       total time =    4427.17 ms /    85 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"42 mph on a good day.\\n\\n\\nUser: What do you think of me?\\nAI: That's like asking a fish what it thinks of water!\\n\\n\\nUser: What's your name?\\nAI: Yo Momma!\\n\\n\\nUser: Are you male or female?\\nAI: It depends on your definition of male or female! \\n\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(prompt.format(query=\"How fast can an ant fly?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Output parsers](https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description='question to setup the joke')\n",
    "    punchline: str = Field(description='the ansker to resolve the joke')\n",
    "\n",
    "    @validator(\"setup\")\n",
    "    def question_ends_with_question_mark(cls, field):\n",
    "        if field[-1] != '?':\n",
    "            raise ValueError(\"Question ought to end with a '?'\")\n",
    "        return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = PydanticOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Just answer the query.\\n{format_instructions}\\n{query}\",\n",
    "    input_variables=['query'],\n",
    "    partial_variables={\n",
    "        'format_instructions': parser.get_format_instructions()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     407.06 ms\n",
      "llama_print_timings:      sample time =      25.40 ms /   292 runs   (    0.09 ms per token, 11496.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   13841.85 ms /   292 runs   (   47.40 ms per token,    21.10 tokens per second)\n",
      "llama_print_timings:       total time =   14294.67 ms /   293 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  ♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬♬\n"
     ]
    }
   ],
   "source": [
    "# chain\n",
    "# And a query intended to prompt a language model to populate the data structure.\n",
    "chain = prompt | llm\n",
    "output = chain.invoke({\n",
    "    'query': 'Be funny to me.'\n",
    "})\n",
    "print(\"output: \" + output)\n",
    "# parser.invoke(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup='Why did the chicken cross the road?', punchline='To get to the other side!')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Joke(setup='Why did the chicken cross the road?', punchline='To get to the other side!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (in this case foo is the name of your variable). If you pass in more than 5 values then it will add to the end of the list with commas."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     407.06 ms\n",
      "llama_print_timings:      sample time =       3.15 ms /    36 runs   (    0.09 ms per token, 11439.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1147.88 ms /    29 tokens (   39.58 ms per token,    25.26 tokens per second)\n",
      "llama_print_timings:        eval time =    1700.58 ms /    35 runs   (   48.59 ms per token,    20.58 tokens per second)\n",
      "llama_print_timings:       total time =    2914.11 ms /    64 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['(in this case foo is the name of your variable). If you pass in more than 5 values then it will add to the end of the list with commas.']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"subject\": \"ice cream flavors\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your response should be a list of comma separated values, eg: `foo, bar, baz`'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser, CommaSeparatedListOutputParser\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List five {subject}\",\n",
    "    input_variables=['query']\n",
    ")\n",
    "parser = CommaSeparatedListOutputParser()\n",
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain = prompt | llm | StrOutputParser()\n",
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " you love the most. савез\n",
      "List five ice cream flavors you love the most\n",
      "The ice cream cone is a must at any celebration, especially if it’s one that involves cake and presents for children of all ages! If you can’t find a place where they sell their own ice creams, head to the grocery store or supermarket. A good idea would be to get some of your favorite ice creams and place them in different bowls around the house as an option for guests to choose from (just make sure there are no allergens present). This will make sure everyone has something they like while also keeping things interesting since no two people will have exactly the same flavor combination!\n",
      "What is your favorite type of ice cream?\n",
      "My favorite type of ice cream would have to be mint chocolate chip because I love how refreshing mint tastes with chocolate chips on top! If I had to pick another type though…maybe cookies ’n cream? It’s such an interesting combination but still very simple – perfect if you want something easy but also delicious (not just sweet). There are many different kinds of ice creams out there but these two will always be my favorites!\n",
      "What’s your favorite type of ice cream?\n",
      "Ice cream is my favorite treat because it has such an interesting flavor combination – no matter what type I choose, I know I will enjoy every bite! And even though there are many different kinds out there…I always seem to gravitate towards two particular ones: mint chocolate chip (for how refreshingly mint tastes) or cookies ’n cream (for how interesting this combo actually tastes). But no matter what type I choose – I always know I will enjoy every bite because there are so many different kinds out there! You just need to make sure your grocery store stocks whatever your favorite brand might be before deciding which type you want…but once you know what kind goes best with those other flavors like strawberry or chocolate fudge…then everything else should be easy peasy lemon squeezy 🙂"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     407.06 ms\n",
      "llama_print_timings:      sample time =      37.11 ms /   464 runs   (    0.08 ms per token, 12503.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =     223.97 ms /     5 tokens (   44.79 ms per token,    22.32 tokens per second)\n",
      "llama_print_timings:        eval time =   22781.63 ms /   463 runs   (   49.20 ms per token,    20.32 tokens per second)\n",
      "llama_print_timings:       total time =   23889.90 ms /   468 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['you love the most. савез\\nList five ice cream flavors you love the most\\nThe ice cream cone is a must at any celebration',\n",
       " 'especially if it’s one that involves cake and presents for children of all ages! If you can’t find a place where they sell their own ice creams',\n",
       " 'head to the grocery store or supermarket. A good idea would be to get some of your favorite ice creams and place them in different bowls around the house as an option for guests to choose from (just make sure there are no allergens present). This will make sure everyone has something they like while also keeping things interesting since no two people will have exactly the same flavor combination!\\nWhat is your favorite type of ice cream?\\nMy favorite type of ice cream would have to be mint chocolate chip because I love how refreshing mint tastes with chocolate chips on top! If I had to pick another type though…maybe cookies ’n cream? It’s such an interesting combination but still very simple – perfect if you want something easy but also delicious (not just sweet). There are many different kinds of ice creams out there but these two will always be my favorites!\\nWhat’s your favorite type of ice cream?\\nIce cream is my favorite treat because it has such an interesting flavor combination – no matter what type I choose',\n",
       " 'I know I will enjoy every bite! And even though there are many different kinds out there…I always seem to gravitate towards two particular ones: mint chocolate chip (for how refreshingly mint tastes) or cookies ’n cream (for how interesting this combo actually tastes). But no matter what type I choose – I always know I will enjoy every bite because there are so many different kinds out there! You just need to make sure your grocery store stocks whatever your favorite brand might be before deciding which type you want…but once you know what kind goes best with those other flavors like strawberry or chocolate fudge…then everything else should be easy peasy lemon squeezy 🙂']"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"subject\": \"ice cream flavors\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\n",
      "\n",
      "Examples: 0423-04-15T09:40:38.384214Z, 1270-06-05T17:12:34.477661Z, 1611-11-21T02:29:31.220390Z\n",
      "\n",
      "Return ONLY this string, no other words!\n"
     ]
    }
   ],
   "source": [
    "datetime_parser = DatetimeOutputParser(Hformat=\"%a %d/%m/%Y\")\n",
    "print(datetime_parser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"what date and time did this happen? {question}\\n{format_instructions}\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='what date and time did this happen? world war 2?\\nYour response should be a list of comma separated values, eg: `foo, bar, baz`')"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format_prompt(question='world war 2?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | datetime_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Comment: @Jimbo it was 6 hours ago\n",
      "\n",
      "Answer: You have the following issue. You are trying to insert the current timestamp in UTC into the database which is stored in UTC but you need to convert it into your local time zone first.\n",
      "\n",
      "Use `NOW()` with your local timezone instead of UTC like below for your example code in `create_table_3`:\n",
      "\n",
      "\\begin{code}\n",
      "NOW('localtime')\n",
      "\\end{code}\n",
      "\n",
      "Comment: I still got the same error.\n",
      "\n",
      "Comment: What version of MySQL is that on ?"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     407.06 ms\n",
      "llama_print_timings:      sample time =      10.60 ms /   127 runs   (    0.08 ms per token, 11977.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1026.08 ms /    26 tokens (   39.46 ms per token,    25.34 tokens per second)\n",
      "llama_print_timings:        eval time =    6176.74 ms /   126 runs   (   49.02 ms per token,    20.40 tokens per second)\n",
      "llama_print_timings:       total time =    7429.73 ms /   152 tokens\n"
     ]
    },
    {
     "ename": "OutputParserException",
     "evalue": "Could not parse datetime string: \n\nComment: @Jimbo it was 6 hours ago\n\nAnswer: You have the following issue. You are trying to insert the current timestamp in UTC into the database which is stored in UTC but you need to convert it into your local time zone first.\n\nUse `NOW()` with your local timezone instead of UTC like below for your example code in `create_table_3`:\n\n\\begin{code}\nNOW('localtime')\n\\end{code}\n\nComment: I still got the same error.\n\nComment: What version of MySQL is that on ?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llama/lib/python3.10/site-packages/langchain/output_parsers/datetime.py:51\u001b[0m, in \u001b[0;36mDatetimeOutputParser.parse\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llama/lib/python3.10/_strptime.py:568\u001b[0m, in \u001b[0;36m_strptime_datetime\u001b[0;34m(cls, data_string, format)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return a class cls instance based on the input string and the\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;124;03mformat string.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 568\u001b[0m tt, fraction, gmtoff_fraction \u001b[38;5;241m=\u001b[39m \u001b[43m_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m tzname, gmtoff \u001b[38;5;241m=\u001b[39m tt[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llama/lib/python3.10/_strptime.py:349\u001b[0m, in \u001b[0;36m_strptime\u001b[0;34m(data_string, format)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m found:\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime data \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m does not match format \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    350\u001b[0m                      (data_string, \u001b[38;5;28mformat\u001b[39m))\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_string) \u001b[38;5;241m!=\u001b[39m found\u001b[38;5;241m.\u001b[39mend():\n",
      "\u001b[0;31mValueError\u001b[0m: time data \"Comment: @Jimbo it was 6 hours ago\\n\\nAnswer: You have the following issue. You are trying to insert the current timestamp in UTC into the database which is stored in UTC but you need to convert it into your local time zone first.\\n\\nUse `NOW()` with your local timezone instead of UTC like below for your example code in `create_table_3`:\\n\\n\\\\begin{code}\\nNOW('localtime')\\n\\\\end{code}\\n\\nComment: I still got the same error.\\n\\nComment: What version of MySQL is that on ?\" does not match format '%Y-%m-%dT%H:%M:%S.%fZ'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGuns and roses\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llama/lib/python3.10/site-packages/langchain_core/runnables/base.py:2034\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2033\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2034\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2035\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2036\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2037\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2038\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2039\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2040\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2041\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2042\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llama/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:176\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m    169\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llama/lib/python3.10/site-packages/langchain_core/runnables/base.py:1235\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1232\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m   1233\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1234\u001b[0m         Output,\n\u001b[0;32m-> 1235\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1238\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1239\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1243\u001b[0m     )\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1245\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llama/lib/python3.10/site-packages/langchain_core/runnables/config.py:326\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    325\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 326\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llama/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:177\u001b[0m, in \u001b[0;36mBaseOutputParser.invoke.<locals>.<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_result(\n\u001b[1;32m    169\u001b[0m             [ChatGeneration(message\u001b[38;5;241m=\u001b[39minner_input)]\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    174\u001b[0m     )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m--> 177\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    179\u001b[0m         config,\n\u001b[1;32m    180\u001b[0m         run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    181\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llama/lib/python3.10/site-packages/langchain_core/output_parsers/base.py:219\u001b[0m, in \u001b[0;36mBaseOutputParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, result: List[Generation], \u001b[38;5;241m*\u001b[39m, partial: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse a list of candidate model Generations into a specific format.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    The return value is parsed from only the first Generation in the result, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m        Structured output.\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/llama/lib/python3.10/site-packages/langchain/output_parsers/datetime.py:53\u001b[0m, in \u001b[0;36mDatetimeOutputParser.parse\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m datetime\u001b[38;5;241m.\u001b[39mstrptime(response\u001b[38;5;241m.\u001b[39mstrip(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not parse datetime string: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOutputParserException\u001b[0m: Could not parse datetime string: \n\nComment: @Jimbo it was 6 hours ago\n\nAnswer: You have the following issue. You are trying to insert the current timestamp in UTC into the database which is stored in UTC but you need to convert it into your local time zone first.\n\nUse `NOW()` with your local timezone instead of UTC like below for your example code in `create_table_3`:\n\n\\begin{code}\nNOW('localtime')\n\\end{code}\n\nComment: I still got the same error.\n\nComment: What version of MySQL is that on ?"
     ]
    }
   ],
   "source": [
    "chain.invoke({'question':'Guns and roses'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
